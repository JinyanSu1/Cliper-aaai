Running: python ../../generate3dims.py                             --cache_dir '/share/nikola/js3673/cache'                             --tokenizer_model 'TheBloke/tulu-7B-fp16'                             --classifier_tokenizer_model 'JackFram/llama-160m'                             --classifier_model_path '../../models/classifier/llama160m_10000'                             --dataset_path '../../eval/data'                             --dataset alpaca_evaluation100                             --output_dir ../../eval/baselines/classifier_guided                             --alphas 1 1                             --preference_symbols P1A P2A                             --batch_size 8                             --max_new_tokens 512                             --temperature 0.1                             --num_return_sequences 1                             --device 'cuda'                             --seed 42 --use_personalized_prompt --mixed_preference_prompt
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.43s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.15s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.34s/it]
  0%|          | 0/13 [00:00<?, ?it/s]We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)
